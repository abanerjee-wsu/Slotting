{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019 dataset shape:  (571140, 28)\n",
      "----- Feature Set ----- \n",
      " ['UniqueId', 'Department', 'Category', 'SubCategory', 'Material', 'Style', 'Dimension', 'ProductSize', 'Volume', 'InventorySegment', 'Color', 'UPC', 'Season_SKU_Introduced', 'Year_SKU_Introduced', 'History_Size_Pct', 'Month', 'Season', 'Year', 'Quarter', 'PlannedUnits', 'PDXPlannedUnits', 'Orders', 'Ordered_Units', 'Avg_Daily_Ordered_Units', 'Days_Ordered', 'Order_Volume', 'Avg_Daily_Volume', 'Experiment_Id']\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset for 2019 data\n",
    "df2019 = pd.read_csv(r'C:\\Users\\abandyopadhyay\\ALOKPARNA\\RCV_SLOTTING\\Slotting_csv_monthly\\Slotting_2019.csv', header=0, encoding = 'unicode_escape')\n",
    "\n",
    "# Load the dataset for UniqueId mapping\n",
    "dfUnique = pd.read_csv(r'C:\\Users\\abandyopadhyay\\ALOKPARNA\\RCV_SLOTTING\\Slotting_csv_monthly\\Slotting_uniqueId.csv', header=0, encoding = 'unicode_escape')\n",
    "\n",
    "# Join the 2 datasets to create a Unique id dataset for 2019\n",
    "df2019 = df2019.merge(dfUnique, left_on='UniqueId', right_on='UniqueId')\n",
    "\n",
    "print(\"2019 dataset shape: \", df2019.shape)\n",
    "print(\"----- Feature Set ----- \\n\", list(df2019.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the combined 2019 dataframe into a csv file\n",
    "df2019.to_csv(r'C:\\Users\\abandyopadhyay\\ALOKPARNA\\RCV_SLOTTING\\Dataset\\Slotting_2019.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 dataset shape:  (571140, 28)\n",
      "----- Feature Set ----- \n",
      " ['UniqueId', 'Department', 'Category', 'SubCategory', 'Material', 'Style', 'Dimension', 'ProductSize', 'Volume', 'InventorySegment', 'Color', 'UPC', 'Season_SKU_Introduced', 'Year_SKU_Introduced', 'History_Size_Pct', 'Month', 'Season', 'Year', 'Quarter', 'PlannedUnits', 'PDXPlannedUnits', 'Orders', 'Ordered_Units', 'Avg_Daily_Ordered_Units', 'Days_Ordered', 'Order_Volume', 'Avg_Daily_Volume', 'Experiment_Id']\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset for 2018 data\n",
    "df2018 = pd.read_csv(r'C:\\Users\\abandyopadhyay\\ALOKPARNA\\RCV_SLOTTING\\Slotting_csv_monthly\\Slotting_2018.csv', header=0, encoding = 'unicode_escape')\n",
    "\n",
    "# Load the dataset for UniqueId mapping\n",
    "dfUnique = pd.read_csv(r'C:\\Users\\abandyopadhyay\\ALOKPARNA\\RCV_SLOTTING\\Slotting_csv_monthly\\Slotting_uniqueId.csv', header=0, encoding = 'unicode_escape')\n",
    "\n",
    "# Join the 2 datasets to create a Unique id dataset for 2019\n",
    "df2018 = df2018.merge(dfUnique, left_on='UniqueId', right_on='UniqueId')\n",
    "\n",
    "print(\"2018 dataset shape: \", df2018.shape)\n",
    "print(\"----- Feature Set ----- \\n\", list(df2018.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the combined 2018 dataframe into a csv file\n",
    "df2018.to_csv(r'C:\\Users\\abandyopadhyay\\ALOKPARNA\\RCV_SLOTTING\\Dataset\\Slotting_2018.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017 dataset shape:  (571140, 28)\n",
      "----- Feature Set ----- \n",
      " ['UniqueId', 'Department', 'Category', 'SubCategory', 'Material', 'Style', 'Dimension', 'ProductSize', 'Volume', 'InventorySegment', 'Color', 'UPC', 'Season_SKU_Introduced', 'Year_SKU_Introduced', 'History_Size_Pct', 'Month', 'Season', 'Year', 'Quarter', 'PlannedUnits', 'PDXPlannedUnits', 'Orders', 'Ordered_Units', 'Avg_Daily_Ordered_Units', 'Days_Ordered', 'Order_Volume', 'Avg_Daily_Volume', 'Experiment_Id']\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset for 2017 data\n",
    "df2017 = pd.read_csv(r'C:\\Users\\abandyopadhyay\\ALOKPARNA\\RCV_SLOTTING\\Slotting_csv_monthly\\Slotting_2017.csv', header=0, encoding = 'unicode_escape')\n",
    "\n",
    "# Load the dataset for UniqueId mapping\n",
    "dfUnique = pd.read_csv(r'C:\\Users\\abandyopadhyay\\ALOKPARNA\\RCV_SLOTTING\\Slotting_csv_monthly\\Slotting_uniqueId.csv', header=0, encoding = 'unicode_escape')\n",
    "\n",
    "# Join the 2 datasets to create a Unique id dataset for 2019\n",
    "df2017 = df2017.merge(dfUnique, left_on='UniqueId', right_on='UniqueId')\n",
    "\n",
    "print(\"2017 dataset shape: \", df2017.shape)\n",
    "print(\"----- Feature Set ----- \\n\", list(df2017.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the combined 2018 dataframe into a csv file\n",
    "df2017.to_csv(r'C:\\Users\\abandyopadhyay\\ALOKPARNA\\RCV_SLOTTING\\Dataset\\Slotting_2017.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape:  (1713420, 28)\n",
      " \n",
      "\n",
      "----- Feature Set ----- \n",
      " ['UniqueId', 'Department', 'Category', 'SubCategory', 'Material', 'Style', 'Dimension', 'ProductSize', 'Volume', 'InventorySegment', 'Color', 'UPC', 'Season_SKU_Introduced', 'Year_SKU_Introduced', 'History_Size_Pct', 'Month', 'Season', 'Year', 'Quarter', 'PlannedUnits', 'PDXPlannedUnits', 'Orders', 'Ordered_Units', 'Avg_Daily_Ordered_Units', 'Days_Ordered', 'Order_Volume', 'Avg_Daily_Volume', 'Experiment_Id']\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets for 2016 2017 and 2018 data\n",
    "df2016 = pd.read_csv(r'C:\\Users\\abandyopadhyay\\ALOKPARNA\\RCV_SLOTTING\\Slotting_csv_monthly\\Slotting_2016.csv', header=0, encoding = 'unicode_escape')\n",
    "df2017 = pd.read_csv(r'C:\\Users\\abandyopadhyay\\ALOKPARNA\\RCV_SLOTTING\\Slotting_csv_monthly\\Slotting_2017.csv', header=0, encoding = 'unicode_escape')\n",
    "df2018 = pd.read_csv(r'C:\\Users\\abandyopadhyay\\ALOKPARNA\\RCV_SLOTTING\\Slotting_csv_monthly\\Slotting_2018.csv', header=0, encoding = 'unicode_escape')\n",
    "\n",
    "# Create the training dataset with data from 2016-2018\n",
    "dfTrain = df2016.append([df2017, df2018])\n",
    "\n",
    "# Load the dataset for UniqueId mapping\n",
    "dfUnique = pd.read_csv(r'C:\\Users\\abandyopadhyay\\ALOKPARNA\\RCV_SLOTTING\\Slotting_csv_monthly\\Slotting_uniqueId.csv', header=0, encoding = 'unicode_escape')\n",
    "\n",
    "# Join the 2 datasets to create a Unique id dataset for training\n",
    "dfTrain = dfTrain.merge(dfUnique, left_on='UniqueId', right_on='UniqueId')\n",
    "\n",
    "print(\"Training dataset shape: \", dfTrain.shape)\n",
    "print(\" \\n\\n----- Feature Set ----- \\n\", list(dfTrain.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the combined training dataframe into a csv file\n",
    "dfTrain.to_csv(r'C:\\Users\\abandyopadhyay\\ALOKPARNA\\RCV_SLOTTING\\Dataset\\Slotting_Train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset shape:  (2284560, 28)\n",
      " \n",
      "\n",
      "----- Feature Set ----- \n",
      " ['UniqueId', 'Department', 'Category', 'SubCategory', 'Material', 'Style', 'Dimension', 'ProductSize', 'Volume', 'InventorySegment', 'Color', 'UPC', 'Season_SKU_Introduced', 'Year_SKU_Introduced', 'History_Size_Pct', 'Month', 'Season', 'Year', 'Quarter', 'PlannedUnits', 'PDXPlannedUnits', 'Orders', 'Ordered_Units', 'Avg_Daily_Ordered_Units', 'Days_Ordered', 'Order_Volume', 'Avg_Daily_Volume', 'Experiment_Id']\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets for 2016 2017 2018 and 2019 data\n",
    "df2016 = pd.read_csv(r'C:\\Users\\abandyopadhyay\\ALOKPARNA\\RCV_SLOTTING\\Slotting_csv_monthly\\Slotting_2016.csv', header=0, encoding = 'unicode_escape')\n",
    "df2017 = pd.read_csv(r'C:\\Users\\abandyopadhyay\\ALOKPARNA\\RCV_SLOTTING\\Slotting_csv_monthly\\Slotting_2017.csv', header=0, encoding = 'unicode_escape')\n",
    "df2018 = pd.read_csv(r'C:\\Users\\abandyopadhyay\\ALOKPARNA\\RCV_SLOTTING\\Slotting_csv_monthly\\Slotting_2018.csv', header=0, encoding = 'unicode_escape')\n",
    "df2019 = pd.read_csv(r'C:\\Users\\abandyopadhyay\\ALOKPARNA\\RCV_SLOTTING\\Slotting_csv_monthly\\Slotting_2019.csv', header=0, encoding = 'unicode_escape')\n",
    "\n",
    "# Create the training dataset with data from 2016-2018\n",
    "dfFourYears = df2016.append([df2017, df2018, df2019])\n",
    "\n",
    "# Load the dataset for UniqueId mapping\n",
    "dfUnique = pd.read_csv(r'C:\\Users\\abandyopadhyay\\ALOKPARNA\\RCV_SLOTTING\\Slotting_csv_monthly\\Slotting_uniqueId.csv', header=0, encoding = 'unicode_escape')\n",
    "\n",
    "# Join the 2 datasets to create a Unique id dataset for training\n",
    "dfFourYears = dfFourYears.merge(dfUnique, left_on='UniqueId', right_on='UniqueId')\n",
    "\n",
    "print(\"Combined dataset shape: \", dfFourYears.shape)\n",
    "print(\" \\n\\n----- Feature Set ----- \\n\", list(dfFourYears.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the combined training dataframe into a csv file\n",
    "dfFourYears.to_csv(r'C:\\Users\\abandyopadhyay\\ALOKPARNA\\RCV_SLOTTING\\Dataset\\Slotting_FourYears.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2019_Top80_kmeans['Cluster4_Label'].value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
